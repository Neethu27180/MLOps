name: ML Pipeline CI/CD

on:
  push:
    branches:
      - main  # Trigger on pushes to the main branch

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9' # Use the Python version you developed with

      - name: Install dependencies
        run: |
          cd tourism_project
          pip install -r requirements.txt

      - name: Execute Data Preparation and Model Training
        run: |
          cd tourism_project
          # Assuming you have a script or notebook that runs these steps
          # For this example, we'll simulate running the steps from the notebook
          # In a real scenario, you would have dedicated Python scripts
          python -c "
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import mlflow
import joblib
from huggingface_hub import HfApi, hf_hub_download
import os
from google.colab import userdata # Note: userdata won't work in GitHub Actions, replace with env variable

# Use environment variable for HF_TOKEN in GitHub Actions
hf_token = os.environ.get('HF_TOKEN')
if not hf_token:
    raise ValueError('HF_TOKEN environment variable not set.')


# --- Data Loading and Cleaning ---
repo_id = 'Neethu2718/Visit_with_us'
local_data_path = 'data_registration'
os.makedirs(local_data_path, exist_ok=True)

api = HfApi(token=hf_token)

for filename in ['tourism.csv']: # Load the original data for cleaning
    api.hf_hub_download(
        repo_id=repo_id,
        filename=filename,
        repo_type='space',
        local_dir=local_data_path,
        local_dir_use_symlinks=False
    )

df = pd.read_csv(f'{local_data_path}/tourism.csv')

# Data Cleaning (same as in notebook)
irrelevant_columns = ['Unnamed: 0', 'CustomerID']
df = df.drop(columns=irrelevant_columns)

# --- Data Splitting ---
X = df.drop('ProdTaken', axis=1)
y = df['ProdTaken']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Data Preprocessing and Transformation ---
categorical_cols = X_train.select_dtypes(include=['object']).columns
numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns

X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)
X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)

train_cols = X_train.columns
test_cols = X_test.columns
missing_in_test = set(train_cols) - set(test_cols)
for c in missing_in_test:
    X_test[c] = 0
missing_in_train = set(test_cols) - set(train_cols)
for c in missing_in_train:
    X_train[c] = 0
X_test = X_test[train_cols]

scaler = StandardScaler()
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

# Save the scaler for later use in the app
os.makedirs('model_building', exist_ok=True)
joblib.dump(scaler, 'model_building/scaler.joblib')


# --- Model Training ---
model = LogisticRegression(random_state=42, max_iter=5000) # Increased max_iter
model.fit(X_train, y_train.values.ravel())


# --- Model Evaluation ---
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1-score: {f1}')
print(f'ROC AUC: {roc_auc}')


# --- MLflow Logging ---
# MLflow tracking URI can be set here if using a remote server
# mlflow.set_tracking_uri('YOUR_MLFLOW_TRACKING_URI')
with mlflow.start_run(run_name='Logistic Regression Training'):
    mlflow.log_param('solver', 'lbfgs')
    mlflow.log_param('random_state', 42)
    mlflow.log_param('max_iter', 5000) # Log the increased max_iter

    mlflow.log_metric('accuracy', accuracy)
    mlflow.log_metric('precision', precision)
    mlflow.log_metric('recall', recall)
    mlflow.log_metric('f1', f1)
    mlflow.log_metric('roc_auc', roc_auc)

    # Log the model as an MLflow artifact
    mlflow.sklearn.log_model(model, 'logistic_regression_model')

print('MLflow logging completed.')


# --- Save Model and Upload to Hugging Face ---
local_model_path = 'model_building/logistic_regression_model.joblib'
joblib.dump(model, local_model_path)

api.upload_file(
    path_or_fileobj=local_model_path,
    path_in_repo='logistic_regression_model.joblib',
    repo_id=repo_id,
    repo_type='space',
)

print(f'Model successfully saved locally and uploaded to Hugging Face Space: {repo_id}')

# Upload the scaler as well
api.upload_file(
    path_or_fileobj='model_building/scaler.joblib',
    path_in_repo='scaler.joblib',
    repo_id=repo_id,
    repo_type='space',
)
print(f'Scaler successfully uploaded to Hugging Face Space: {repo_id}')

"

        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }} # Use the GitHub Secret

      - name: Make deploy script executable
        run: |
          cd tourism_project
          chmod +x deploy_to_space.sh

      - name: Deploy to Hugging Face Space
        run: |
          cd tourism_project
          ./deploy_to_space.sh
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }} # Pass the GitHub Secret to the script
